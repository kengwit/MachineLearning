{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons why we are not satisfied with the least squares estimates:\n",
    "\n",
    "- *Prediction accuracy*: The least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coefficients to zero.\n",
    "\n",
    "- *Interpretation*: In order to get the \"big picture\", we are willing to sacrifice the small details by selecting a smaller subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Best-Subset Selection\n",
    "\n",
    "Best subset regression finds for each k the subset of size k that givest smallest RSS. An efficient algorithm - the *leaps and bounds* procedure (feasible for p ~ 30, 40). The question of how to choose k involves the tradeoff between bias and variance; typically we choose the smallest model that minimizes an EPE.\n",
    "\n",
    "**TODO**: implement FIGURE 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Forward- and Backward-Stepwise Selection\n",
    "\n",
    "Rather than search through all possible subset (which is infeasible for p > 40), we can seek a good path through them.\n",
    "\n",
    "**Forward-stepwise selection**.\n",
    "\n",
    "Forward-stepwise selection starts with the intercept, and sequentially adds the predictor that most improves the fit. \n",
    "Clever updating algorithms can exploit the QR decomposition for the current fit to rapidly establish the next candidate. List best-subset regression, the subset size k must be determined.\n",
    "\n",
    "The Forward-stepwise selection is a *greedy algorithm*, however, there are reasons why it might be preferred:\n",
    "- *Computational*: for large p we cannot compute the best subset sequence.\n",
    "- *Statistical*: will have lower variance, but perhaps more bias.\n",
    "\n",
    "**Backward-stepwise selection**.\n",
    "\n",
    "Backward-stepwise selection starts with the full model, and sequentially deletes the predictors that has the least impact on the fit (i.e the smallest Z-score). Backward selection can be used only when N > p.\n",
    "\n",
    "**Hybrid stepwise selection**.\n",
    "\n",
    "Some software packages implement hybrid stepwise-selection strategies that consider both forward and backward at each step, and select the best of the two (i.e in the R package the \"step\" function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Forward-Stagewise Regression\n",
    "\n",
    "Forward-Stagewise Regression(FS) is even more constrained than forward-stepwise regression. It starts with an intercept equal to $\\overline{y}$, and centered predictors with coefficients intally all 0. At each step it identifies the variable most correlated with the current residual. It then computes the simple linear regression coefficient of the residual on this chosen variable, and then adds it to the current coefficient for that variable. This is continue till none of variables have correlation with the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.4 Prostate Cancer Data Example \n",
    "\n",
    "**TODO:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
