{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4.1 Fitting Logistic Regression Models\n",
    "\n",
    "Logistic regression models fit by maximum likelihood, the log-likelihood for N observations is (4.19):\n",
    "$$\n",
    "l(\\theta)=\\sum_{i=1}^N \\log p_{g_i}(x_i, \\theta),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discuss in detail the two-class case, it is convenient to code the two-class $g_i$ via 0/1 response $y_i$. The log-likelihood can be written (4.20):\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\beta) &= \\sum_{i=1}^N \\left \\{y_i \\log p(x_i; \\beta) + (1-y_i)log(1 - p(x_i;\\beta)) \\right \\}\\\\\n",
    "&= \\sum_{i=1}^N \\left \\{ \n",
    "  y_i(\\beta^Tx_i - log(1+e^{\\beta^Tx_i})) -\n",
    "  (1-y_i)log(1+e^{\\beta^Tx_i})\n",
    "\\right\\}\\\\\n",
    "&= \\sum_{i=1}^N \\left \\{ y_i\\beta^Tx_i - \\log (1+e^{\\beta^Tx_i}) \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here $\\beta = \\{\\beta_{10}, \\beta_1\\}$, and we assume that the vector of inputs $x_i$ includes the constant term 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the log-likelihood, we set its derivative to zero (4.21):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial l(\\beta)}{\\partial \\beta} &=\\cfrac{\\partial \\left [\n",
    "\\sum_{i=1}^N \\left \\{ y_i\\beta^Tx_i - \\log (1+e^{\\beta^Tx_i}) \\right\\}\n",
    "\\right]}\n",
    "{\\partial \\beta}\\\\\n",
    "&= \\sum_{i=1}^N \\left \\{ y_ix_i - \\cfrac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}   x_i\\right\\}\\\\\n",
    "&= \\sum_{i=1}^N x_i(y_i-p(x_i; \\beta))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which are p + 1 equations *nonlinear* in $\\beta$. Notice that since the first component of $x_i$ is 1, the first score equation specifies that $\\sum_{i=1}^N y_i = \\sum_{i=1}^N p(x_i;\\beta)$, the *expected* number of class ones matches the observed number (ane hence also class twos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the score equation (4.21), we use the Newton-Raphson algorithm, which requires the second-derivatie or Hessian Matrix (4.22):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T} \n",
    "&= \\cfrac{\\partial \\left[ \\sum_{i=1}^N x_iy_i - x_i\\cfrac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}\\right]}{ \\partial \\beta^T}\\\\\n",
    "&= \\cfrac{\\partial \\left[ \\sum_{i=1}^N -x_i\\cfrac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}\\right]}{ \\partial \\beta^T}\\\\\n",
    "&= \\sum_{i=1}^N -x_i x_i^T \n",
    "   \\cfrac{ \n",
    "     e^{\\beta^Tx_i}(1+e^{\\beta^Tx_i}) - e^{2\\beta^Tx_i}\n",
    "   }{\n",
    "     (1+e^{\\beta^Tx_i})^2\n",
    "   }\\\\\n",
    "&= \\sum_{i=1}^N -x_i x_i^T \n",
    "\\cfrac{ e^{\\beta^Tx_i} }{ 1+e^{\\beta^Tx_i} }\n",
    "\\cfrac{ 1 }{ 1+e^{\\beta^Tx_i} }\\\\\n",
    "&= -\\sum_{i=1}^N x_ix_i^Tp(x_i;\\beta)(1-p(x_i;\\beta))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single Newton update is (4.23):\n",
    "$$\n",
    "\\beta^{new}=\\beta^{old} - \n",
    "\\left (\n",
    "  \\cfrac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\n",
    "\\right)^{-1}\n",
    "\\cfrac{\\partial l(\\beta)}{\\partial \\beta}\n",
    "$$\n",
    "\n",
    "We can write it as (4.24, 4.25):\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\cfrac{\\partial l(\\beta)}{\\partial \\beta} = \\mathbf{X}^T(\\mathbf{y}-\\mathbf{p})\\\\\n",
    "\\cfrac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T} = -\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\n",
    "\\end{equation}\n",
    "$$\n",
    "where: \n",
    "\n",
    "- **X** - the $N \\times (p + 1)$ input matrix,\n",
    "\n",
    "- **p** - the vector of fitted probabilities.\n",
    "\n",
    "- **W** - a $N \\times N$ diagonal matrix with ith element $p(x_i, \\beta_{old})(1-p(x_i;\\beta_{old}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton step is thus (4.26):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^{new} \n",
    "&= \\beta^{old} + (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T(\\mathbf{y}-\\mathbf{p})\\\\\n",
    "&= (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W}\n",
    "\\left(\\mathbf{X}\\beta^{old}+\\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p})\\right)\\\\\n",
    "&= (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W}\\mathbf{z}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We re-expressed the Newton step as a weighted least squares step, with the response (4.27):\n",
    "$$\n",
    "\\mathbf{z}=\\mathbf{X}\\beta^{old}+\\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p})\n",
    "$$\n",
    "\n",
    "also known as the *adjusted response*. These equations get solved repeatedly and referred to as *iteratively least squares* (IRLS), since each iteration solves the weighted least squares problem(4.28): \n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\underset{\\beta}{argmin} (\\mathbf{z}-\\mathbf{X}\\beta)^T\\mathbf{W}(\\mathbf{z}-\\mathbf{X}\\beta)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
