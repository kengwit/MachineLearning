{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.9 Model Selection and the Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models described have a *smoothing* or *complexity* parameter that has to be determined:\n",
    "\n",
    "- the multiplier of the penalty term;\n",
    "\n",
    "- the width of the kernel\n",
    "\n",
    "- or the number of basis functions\n",
    "\n",
    "We cannont use RSS on the training data to determine these parameters, since we would always pick those that gave interpolating fits and have zero residuals.\n",
    "\n",
    "The kNN regression fit $\\hat{f_k}(x_0)$ illustrates the competing forces that effect the predictive ability of such approximations. Suppose the data arise from a model $Y = f(X) + \\varepsilon$ with $E(\\varepsilon)=0$ and $Var(\\varepsilon) = \\sigma^2$. We assume that the values of $x_i$ in the sample are fixed. The EPE at $x_0$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "EPE_k(x_0) &= E[(Y - \\hat{f_k}(x_0))^2|X=x_0]\\\\\n",
    "&=\\sigma^2 + [Bias^2(\\hat{f_k}(x_0)) + Var_\\tau(\\hat{f_k}(x_0))]\\\\\n",
    "&=\\sigma^2 + \\left[f(x_0) - \\frac{1}{k}\\sum_{l=1}^k{f(x_{(l)})} \\right]^2 + \\frac{\\sigma^2}{k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The subscripts in parentheses ($l$)  indicates the sequence of nearest neighbors to $x_0$. There are three terms in this expression:\n",
    "\n",
    "1. $\\sigma^2$ is the *irreducible* error - is beyond our control, even if we know the true $f(x_0)$.\n",
    "\n",
    "2. The bias term and the expected value of the estimate - $[E_\\tau(\\hat{f_k}(x_0))-f(x_0)]^2$ - where the expected averages the randomness in the training data. This term increases with $k$ if the function is smooth.\n",
    "\n",
    "3. The variance term and it decreases as the inverse of k. The expected value of the variance is: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var_\\tau(\\hat{f_k}(x_0)) &= E_\\tau\\left[\\hat{f_k}(x_0) - E_\\tau(\\hat{f_k}(x_0))\\right]^2\\\\\n",
    "&= E_\\tau\\left[\\frac{1}{k}\\sum_{l=1}^k (f(x_{(l)}) + \\varepsilon_l) - \\frac{1}{k}\\sum_{l=1}^k{f(x_{(l)})}\\right]^2\\\\\n",
    "&= E_\\tau\\left[\\frac{1}{k}\\sum_{l=1}^k \\varepsilon_l\\right]^2\\\\\n",
    "&= \\frac{1}{k^2}E_\\tau\\left[\\sum_{l=1}^k \\varepsilon_l\\right]^2\\\\\n",
    "&= \\frac{1}{k^2}E_\\tau\\left[\\sum_{l=1}^k \\varepsilon_l^2\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As the *model complexity* of our procedure is increased, the variance tends to increase and the squared bias tends to decrease. The opposite behavior occurs as the model complexity is decreased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
